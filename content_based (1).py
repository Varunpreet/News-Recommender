# -*- coding: utf-8 -*-
"""Content_Based

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1de5VUGLy626cNkgjapPln6YZzSL3z9pm
"""

# !pip install surprise

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings('always')
warnings.filterwarnings('ignore')

# data visualisation and manipulation
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import style
import seaborn as sns
 
#configure
# sets matplotlib to inline and displays graphs below the corressponding cell.
# %matplotlib inline  
style.use('fivethirtyeight')
sns.set(style='whitegrid',color_codes=True)

#model selection
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder

#preprocess.
from keras.preprocessing.image import ImageDataGenerator

#dl libraraies
import keras
from keras import backend as K
from keras.models import Sequential
from keras.layers import Dense , merge
from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop
from keras.utils.vis_utils import model_to_dot
from keras.callbacks import ReduceLROnPlateau


from keras.layers.merge import dot
from keras.models import Model


# specifically for deeplearning.
from keras.layers import Dropout, Flatten,Activation,Input,Embedding
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
import tensorflow as tf
import random as rn
from IPython.display import SVG
 
# specifically for manipulating zipped images and getting numpy arrays of pixel values of images.
import cv2                  
import numpy as np  
from tqdm import tqdm
import os                   
from random import shuffle  
from zipfile import ZipFile
from PIL import Image



from surprise import Reader, Dataset, SVD
from surprise.model_selection import cross_validate

import warnings; warnings.simplefilter('ignore')

data = pd.read_csv('/content/news.com.au.csv')

data=data.rename(columns={"Unnamed: 0":"ArticleId_served"})
data.head()

data.shape

x=data.shape[0]

import scipy
import random
from scipy import stats

random.seed(15)

user_session = stats.geom.rvs(size=131,  
                                  p=0.3) 
user_session

count_dict = {x : list(user_session).count(x) for x in user_session}
count_dict

import numpy as np

user_Id = range(1,131)

userId_session = list(zip(user_Id,[10*i for i in user_session]))

sum1 = 0
for i in range(len(userId_session)):
    
    sum1 += userId_session[i][1]
    
sum1

UserIDs = []

for i in range(len(userId_session)):
    
    for j in range(userId_session[i][1]):
        UserIDs.append(userId_session[i][0])

len(UserIDs)

session_list = list(user_session)

session_Id =[]

for i in session_list:
    
    for j in range(1,i+1):
        session_Id.append([j for i in range(10)])

session_Id = np.array(session_Id).flatten()
session_Id.shape

User_session = list(zip(UserIDs,session_Id ))

import pandas as pd

user = pd.DataFrame(User_session, columns=['UserId', 'SessionId'])

user

article_Id = list(range(131))

article_Id = article_Id*int(4200/131)

import random
for x in range(len(User_session)-len(article_Id)):
    article_Id.append(random.randint(1,131))

from random import shuffle
shuffle(article_Id)

c = len(user['UserId'])

article_Id = article_Id[:c]
user['ArticleId_served'] = article_Id

p = len(user['UserId'])
import random
numLow = 1 
numHigh = 6
x = []
for i in range (0,p):
    m = random.sample(range(numLow, numHigh), 1)
    x.append(m)

flat_list = []
for sublist in x:
    for item in sublist:
        flat_list.append(item)

user['rating'] = flat_list
user

df=pd.merge(user,data,on='ArticleId_served')
df

from sklearn.feature_extraction.text import TfidfVectorizer

import nltk,string,re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer 

import warnings
warnings.filterwarnings('always')
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import style
import seaborn as sns

nltk.download('stopwords')
stop_words = stopwords.words('english')
nltk.download('punkt')
nltk.download('all')

df['article']=  df['content'].astype(str)

#tokenize articles to sentences
df['article']=df['article'].apply(lambda x: nltk.sent_tokenize(x))

#tokenize articles sentences to words
df['article']=df['article'].apply(lambda x: [nltk.word_tokenize(sent) for sent in x])

#lower case
df['article']=df['article'].apply(lambda x: [[wrd.lower() for wrd in sent] for sent in x])

#White spaces removal
df['article']=df['article'].apply(lambda x: [[wrd.strip() for wrd in sent if wrd != ' '] for sent in x])

#remove stop words
stopwrds = set(stopwords.words('english'))
df['article']=df['article'].apply(lambda x: [[wrd for wrd in sent if not wrd in stopwrds] for sent in x])

#remove punctuation words
table = str.maketrans('', '', string.punctuation)
df['article']=df['article'].apply(lambda x: [[wrd.translate(table) for wrd in sent] for sent in x])

#remove not alphabetic characters
df['article']=df['article'].apply(lambda x: [[wrd for wrd in sent if wrd.isalpha()] for sent in x])

#lemmatizing article
lemmatizer = WordNetLemmatizer()
df['article']=df['article'].apply(lambda x:[[lemmatizer.lemmatize(wrd.strip()) for wrd in sent ] for sent in x ])

#remove single characters
df['article']= df['article'].apply(lambda x: [[wrd for wrd in sent if len(wrd)>2] for sent in x])

df['article']=df['article'].apply(lambda x:[' '.join(wrd) for wrd in x])
df['article']=df['article'].apply(lambda x:' '.join(x))
print(df['article'][0])

u=pd.DataFrame(columns=['UserId','ArticleId_served','rating'])
u['UserId']=df['UserId'].unique()
u['ArticleId_served']=df['ArticleId_served'].unique()
u['rating']=df['rating']

u

tfidf_vectorizer=TfidfVectorizer(use_idf=True)
tfidf_article=tfidf_vectorizer.fit_transform(df['article'])

top_tf_df = pd.pivot(data=u,index='UserId',columns='ArticleId_served',values='rating')
top_tf_df.fillna(0)

from scipy.sparse import csr_matrix

def create_X(df):

    N = df['UserId'].nunique()
    M = df['ArticleId_served'].nunique()

    user_mapper = dict(zip(np.unique(df['UserId']), list(range(N))))
    news_mapper = dict(zip(np.unique(df['ArticleId_served']), list(range(M))))

    user_inv_mapper = dict(zip(list(range(N)), np.unique(df["UserId"])))
    news_inv_mapper = dict(zip(list(range(M)), np.unique(df["ArticleId_served"])))

    user_index = [user_mapper[i] for i in df['UserId']]
    news_index = [news_mapper[i] for i in df['ArticleId_served']]

    X = csr_matrix((df["rating"], (news_index, user_index)), shape=(M, N))

    return X, user_mapper, news_mapper, user_inv_mapper, news_inv_mapper

X, user_mapper, news_mapper, user_inv_mapper, news_inv_mapper = create_X(u)

# !pip install fuzzywuzzy

from fuzzywuzzy import process

def news_finder(title):
    all_titles = df['title'].tolist()
    closest_match = process.extractOne(title,all_titles)
    return closest_match[0]

news_title_mapper = dict(zip(df['title'], df['ArticleId_served']))
news_title_inv_mapper = dict(zip(df['ArticleId_served'], df['title']))
news_content_mapper=dict(zip(df['title'],df['content']))

def get_news_index(title):
    fuzzy_title = news_finder(title)
    news_id = news_title_mapper[fuzzy_title]
    news_idx = news_mapper[news_id]
    return news_idx

def get_news_title(news_idx):
    news_id = news_inv_mapper[news_idx]
    title = news_title_inv_mapper[news_id]
    return title

def get_content(title):
    fuzzy_title = news_finder(title)
    content = news_content_mapper[fuzzy_title]
    return content

get_news_title(56)

from sklearn.neighbors import NearestNeighbors

import pickle


def find_similar_news(news_id, X, k, metric='cosine', show_distance=False):

    neighbour_ids = []

    news_ind = news_mapper[news_id]
    print(news_ind)
    news_vec = X[news_ind]
    k += 1
    kNN = NearestNeighbors(n_neighbors=k, algorithm="brute", metric=metric)
    import pickle
    # Its important to use binary mode
    knnPickle = open('knnpickle_file', 'wb')
    # source, destination
    pickle.dump(kNN, knnPickle)
    # load the model from disk
    # loaded_model = pickle.load(open('knnpickle_file', 'rb'))
    kNN.fit(X)
    # Save the trained model as a pickle string.

    # Load the pickled model
    if isinstance(news_vec, (np.ndarray)):
        news_vec = news_vec.reshape(1, -1)
    neighbour = kNN.kneighbors(news_vec, return_distance=show_distance)
    for i in range(0, k):
        n = neighbour.item(i)
        neighbour_ids.append(news_inv_mapper[n])
    neighbour_ids.pop(0)
    return neighbour_ids


news_titles = dict(zip(df['ArticleId_served'], df['title']))

news_id = 1
similar_ids = find_similar_news(news_id, X, k=10, metric="euclidean")

news_title = news_titles[news_id]
print(f"Because you watched {news_title}:")
for i in similar_ids:
    print(news_titles[i])
    # df.loc[df['title'] == news_titles[i]]

get_news_index("National abortion rights threatened by new US Supreme Court appointment")

df.loc[df['ArticleId_served'] == 43]

get_content("National abortion rights threatened by new US Supreme Court appointment")